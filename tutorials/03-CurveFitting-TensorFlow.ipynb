{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorials_3-CurveFitting-TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "u6-He3cXBKfM"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Linear Regression - Curve Fitting (TensorFlow)\n",
        "In this tutorial we will use two methods to fit a curve using [TensorFlow](https://www.tensorflow.org/):\n",
        "- Direct solution using the least-squares method - this is the same method used in the previous tutorial with NumPy \n",
        "- Iterative optimisation using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent/)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pU5v5NAxBKfQ"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data\n",
        "First, as before, we sample $n$ observed data from the underlying polynomial defined by weights $w$:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i2T4JW1SBKfU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# get ground-truth data from the \"true\" model \n",
        "n = 100 \n",
        "w = [4, 3, 2, 1]\n",
        "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
        "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
        "                       np.linspace(len(w)-1,0,len(w))), w)\n",
        "std_noise = 0.2\n",
        "t_observed = np.reshape(\n",
        "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
        "    [-1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VmgdFSrvBKfc"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Computation Graph and Session\n",
        "[Graphs and sessions](https://www.tensorflow.org/guide/graphs) are important features of TensorFlow. Briefly:\n",
        "1. a **graph** needs to be built to specify what computations are required; the graphs are made up of nodes (mathematical operations) and edges (multidimensional data arrays, i.e. *tensors*, that are consumed or produced by a computation)\n",
        "1. **sessions** are then constructed to specify what computation to run -  e.g., what data to use and in what order. \n",
        "\n",
        "To facilitate the data feeding, [**placeholders**](https://www.tensorflow.org/api_docs/python/tf/placeholder) are used. The following two methods to fit the model provide two examples of how these are used in practice."
      ]
    },
    {
      "metadata": {
        "id": "Gx0uDEF6rURs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we build a computation graph using \"tf functions\":"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8w3xBytsBKff",
        "outputId": "ad257b69-2dac-485f-dbaa-f197b0073b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# placeholders are used for feeding data in runtime\n",
        "ph_x = tf.placeholder(tf.float32, [n, 1])\n",
        "ph_t = tf.placeholder(tf.float32, [n, 1])\n",
        "\n",
        "deg = 3\n",
        "# define the computation node X\n",
        "node_X = tf.pow(ph_x, tf.linspace(tf.to_float(deg),0,deg+1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-3fee1fcedbf0>:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82XOeCIErURw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This above is a very simple computation graph to evaluate the polynomial using TensorFlow functions. This can be built without any real data and there has not been any computation taking place either.  \n",
        "\n",
        "Then we construct a session. And, call the run method to evaluate the node *node_X* to actually run the computation and obtain the results."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xUHDEl-yBKft"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Least-Squares Solution\n",
        "\n",
        "To begin this curve fitting tutorial, we will start with the same method from the previous NumPy tutorial: the least squares solution. The advantages of using TensorFlow are not obvious with this method, however they will become apparent later for SGD. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DYZu3MyaBKfw",
        "outputId": "c62da439-ffb1-4bad-8c44-0c5f79a8f94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# we complete the computation graph with the least-square solution\n",
        "node_w = tf.matrix_solve_ls(node_X, ph_t)\n",
        "\n",
        "# run the session to evaluate the node weights\n",
        "sess = tf.Session()  \n",
        "dataFeed = {ph_x:x, ph_t:t_observed}  # feed data\n",
        "w_lstsq = sess.run(node_w, feed_dict=dataFeed)\n",
        "print(w_lstsq)\n",
        "#tf.reset_default_graph()\n",
        "sess.close()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.9337618]\n",
            " [2.9645503]\n",
            " [2.0214155]\n",
            " [1.0080402]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qhy2ym8FBKf8"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Stochastic Gradient Descend Method\n",
        "Instead of using least-squares, weights can be optimised by minimising a loss function between the predicted- and observed target values using [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). SGD works by taking the gradient of a random point in our data set and using its value to inform whether to increase or decrease the value of the model weights.\n",
        "\n",
        "It is not an efficient method for this curve fitting problem; this is only for the purpose of demonstrating how an iterative method can be implemented in TensorFlow."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x2TDAsD-BKf_",
        "outputId": "23cdb487-2e5a-45db-d32d-a0a5c3b3b540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1476
        }
      },
      "cell_type": "code",
      "source": [
        "# build a new graph\n",
        "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
        "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
        "\n",
        "deg = 3\n",
        "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
        "\n",
        "# first declare variables that need optimisation\n",
        "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
        "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
        "\n",
        "# complete the computation graph with SGD\n",
        "node_1t = tf.matmul(node_X, var_w)\n",
        "# define a square loss function\n",
        "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
        "# buiding a train-op to minimise the loss\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
        "\n",
        "# launch a session\n",
        "sess = tf.Session()  \n",
        "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
        "\n",
        "# iteration to update variables with backprop gradients\n",
        "total_iter = int(1e4)\n",
        "indices_train = [i for i in range(n)]\n",
        "for step in range(total_iter):\n",
        "\n",
        "    idx = step % n\n",
        "    if idx == 0:  # shuffle every epoch\n",
        "        random.shuffle(indices_train)\n",
        "    \n",
        "    # single data point feed\n",
        "    singleDataFeed = {\n",
        "        ph_1x:x[indices_train[idx],np.newaxis], \n",
        "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
        "    \n",
        "    # print for testing\n",
        "    #print(singleDataFeed)\n",
        "    \n",
        "    # update the variables\n",
        "    sess.run(train_op, feed_dict=singleDataFeed)\n",
        "    \n",
        "    # print training information\n",
        "    if (step % 200) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
        "        print('Step %d: Loss=%f' % (step, loss_train))\n",
        "    if (step % 2000) == 0:\n",
        "        w_sgd = sess.run(var_w)\n",
        "        print('Estimated weights:')\n",
        "        print(w_sgd)\n",
        "\n",
        "w_sgd = sess.run(var_w)\n",
        "print('Final weights at step %d:' % step)\n",
        "print(w_sgd)\n",
        "sess.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Step 0: Loss=0.062582\n",
            "Estimated weights:\n",
            "[[-0.01558511]\n",
            " [ 0.02692418]\n",
            " [-0.04421873]\n",
            " [ 0.06998011]]\n",
            "Step 200: Loss=0.003946\n",
            "Step 400: Loss=0.065428\n",
            "Step 600: Loss=0.010327\n",
            "Step 800: Loss=0.009176\n",
            "Step 1000: Loss=0.069985\n",
            "Step 1200: Loss=0.001761\n",
            "Step 1400: Loss=0.078367\n",
            "Step 1600: Loss=0.000047\n",
            "Step 1800: Loss=0.010099\n",
            "Step 2000: Loss=0.008254\n",
            "Estimated weights:\n",
            "[[3.9506228 ]\n",
            " [3.0424283 ]\n",
            " [2.0201447 ]\n",
            " [0.98608625]]\n",
            "Step 2200: Loss=0.010776\n",
            "Step 2400: Loss=0.024494\n",
            "Step 2600: Loss=0.000001\n",
            "Step 2800: Loss=0.002909\n",
            "Step 3000: Loss=0.000402\n",
            "Step 3200: Loss=0.000030\n",
            "Step 3400: Loss=0.045465\n",
            "Step 3600: Loss=0.023620\n",
            "Step 3800: Loss=0.000852\n",
            "Step 4000: Loss=0.002616\n",
            "Estimated weights:\n",
            "[[3.9118133]\n",
            " [2.8569922]\n",
            " [1.9966732]\n",
            " [1.1143385]]\n",
            "Step 4200: Loss=0.115749\n",
            "Step 4400: Loss=0.068807\n",
            "Step 4600: Loss=0.000024\n",
            "Step 4800: Loss=0.005590\n",
            "Step 5000: Loss=0.287029\n",
            "Step 5200: Loss=0.036952\n",
            "Step 5400: Loss=0.039631\n",
            "Step 5600: Loss=0.008008\n",
            "Step 5800: Loss=0.007115\n",
            "Step 6000: Loss=0.011407\n",
            "Estimated weights:\n",
            "[[4.0160127]\n",
            " [2.9579701]\n",
            " [2.057385 ]\n",
            " [0.9945558]]\n",
            "Step 6200: Loss=0.000931\n",
            "Step 6400: Loss=0.010880\n",
            "Step 6600: Loss=0.050955\n",
            "Step 6800: Loss=0.034287\n",
            "Step 7000: Loss=0.009725\n",
            "Step 7200: Loss=0.004830\n",
            "Step 7400: Loss=0.001092\n",
            "Step 7600: Loss=0.014959\n",
            "Step 7800: Loss=0.002296\n",
            "Step 8000: Loss=0.001061\n",
            "Estimated weights:\n",
            "[[3.9016736 ]\n",
            " [2.9772387 ]\n",
            " [2.0322723 ]\n",
            " [0.98174816]]\n",
            "Step 8200: Loss=0.032781\n",
            "Step 8400: Loss=0.004774\n",
            "Step 8600: Loss=0.005164\n",
            "Step 8800: Loss=0.001421\n",
            "Step 9000: Loss=0.000043\n",
            "Step 9200: Loss=0.005904\n",
            "Step 9400: Loss=0.009613\n",
            "Step 9600: Loss=0.009158\n",
            "Step 9800: Loss=0.002172\n",
            "Final weights at step 9999:\n",
            "[[3.9882133]\n",
            " [2.9249914]\n",
            " [2.1523967]\n",
            " [1.021955 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "db07WaJ0v1Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Playing around with the optimiser\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uDHZJiUGv-X8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build a new graph\n",
        "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
        "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
        "\n",
        "deg = 3\n",
        "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
        "\n",
        "# first declare variables that need optimisation\n",
        "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
        "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
        "\n",
        "# complete the computation graph with SGD\n",
        "node_1t = tf.matmul(node_X, var_w)\n",
        "# define a square loss function\n",
        "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
        "# buiding a train-op to minimise the loss\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
        "\n",
        "# launch a session\n",
        "sess = tf.Session()  \n",
        "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
        "\n",
        "# iteration to update variables with backprop gradients\n",
        "total_iter = int(1e4)\n",
        "indices_train = [i for i in range(n)]\n",
        "for step in range(total_iter):\n",
        "\n",
        "    idx = step % n\n",
        "    if idx == 0:  # shuffle every epoch\n",
        "        random.shuffle(indices_train)\n",
        "    \n",
        "    # single data point feed\n",
        "    singleDataFeed = {\n",
        "        ph_1x:x[indices_train[idx],np.newaxis], \n",
        "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
        "    \n",
        "    # print for testing\n",
        "    #print(singleDataFeed)\n",
        "    \n",
        "    # update the variables\n",
        "    sess.run(train_op, feed_dict=singleDataFeed)\n",
        "    \n",
        "    # print training information\n",
        "    if (step % 200) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
        "        print('Step %d: Loss=%f' % (step, loss_train))\n",
        "    if (step % 2000) == 0:\n",
        "        w_sgd = sess.run(var_w)\n",
        "        print('Estimated weights:')\n",
        "        print(w_sgd)\n",
        "\n",
        "w_sgd = sess.run(var_w)\n",
        "print('Final weights at step %d:' % step)\n",
        "print(w_sgd)\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FxVSxLyeVdXb"
      },
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "- What happens to the convergence when you try other optimisation hyperparameters, such as a different optimiser, learning rate and number of iterations? You could try plotting the loss as a function of step for each hypermarater. \n",
        "- Try adding regularisers and different loss functions. How does this affect the performance?\n",
        "- Would batch gradient descent or minibatch gradient descent improve the optimisation?\n",
        "- Would higher-degree models be more prone to overfitting and why?\n"
      ]
    },
    {
      "metadata": {
        "id": "7BsWVetqrUSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}