{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorials_3-CurveFitting-TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "u6-He3cXBKfM"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Linear Regression - Curve Fitting (TensorFlow)\n",
        "In this tutorial we will use two methods to fit a curve using [TensorFlow](https://www.tensorflow.org/):\n",
        "- Direct solution using the least-squares method - this is the same method used in the previous tutorial with NumPy \n",
        "- Iterative optimisation using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent/)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pU5v5NAxBKfQ"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data\n",
        "First, as before, we sample $n$ observed data from the underlying polynomial defined by weights $w$:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i2T4JW1SBKfU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# get ground-truth data from the \"true\" model \n",
        "n = 100 \n",
        "w = [4, 3, 2, 1]\n",
        "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
        "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
        "                       np.linspace(len(w)-1,0,len(w))), w)\n",
        "std_noise = 0.2\n",
        "t_observed = np.reshape(\n",
        "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
        "    [-1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VmgdFSrvBKfc"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Computation Graph and Session\n",
        "[Graphs and sessions](https://www.tensorflow.org/guide/graphs) are important features of TensorFlow. Briefly:\n",
        "1. a **graph** needs to be built to specify what computations are required; the graphs are made up of nodes (mathematical operations) and edges (multidimensional data arrays, i.e. *tensors*, that are consumed or produced by a computation)\n",
        "1. **sessions** are then constructed to specify what computation to run -  e.g., what data to use and in what order. \n",
        "\n",
        "To facilitate the data feeding, [**placeholders**](https://www.tensorflow.org/api_docs/python/tf/placeholder) are used. The following two methods to fit the model provide two examples of how these are used in practice."
      ]
    },
    {
      "metadata": {
        "id": "Gx0uDEF6rURs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we build a computation graph using \"tf functions\":"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8w3xBytsBKff",
        "outputId": "63428858-60b0-486d-e8f1-af9b5b37d7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# placeholders are used for feeding data in runtime\n",
        "ph_x = tf.placeholder(tf.float32, [n, 1])\n",
        "ph_t = tf.placeholder(tf.float32, [n, 1])\n",
        "\n",
        "deg = 3\n",
        "# define the computation node X\n",
        "node_X = tf.pow(ph_x, tf.linspace(tf.to_float(deg),0,deg+1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-3fee1fcedbf0>:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82XOeCIErURw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This above is a very simple computation graph to evaluate the polynomial using TensorFlow functions. This can be built without any real data and there has not been any computation taking place either.  \n",
        "\n",
        "Then we construct a session. And, call the run method to evaluate the node *node_X* to actually run the computation and obtain the results."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xUHDEl-yBKft"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Least-Squares Solution\n",
        "\n",
        "To begin this curve fitting tutorial, we will start with the same method from the previous NumPy tutorial: the least squares solution. The advantages of using TensorFlow are not obvious with this method, however they will become apparent later for SGD. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DYZu3MyaBKfw",
        "outputId": "3b6f593c-922d-471a-aad8-096f890843fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# we complete the computation graph with the least-square solution\n",
        "node_w = tf.matrix_solve_ls(node_X, ph_t)\n",
        "\n",
        "# run the session to evaluate the node weights\n",
        "sess = tf.Session()  \n",
        "dataFeed = {ph_x:x, ph_t:t_observed}  # feed data\n",
        "w_lstsq = sess.run(node_w, feed_dict=dataFeed)\n",
        "print(w_lstsq)\n",
        "#tf.reset_default_graph()\n",
        "sess.close()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.042428 ]\n",
            " [2.9618015]\n",
            " [1.9706464]\n",
            " [0.9955562]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qhy2ym8FBKf8"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Stochastic Gradient Descend Method\n",
        "Instead of using least-squares, weights can be optimised by minimising a loss function between the predicted- and observed target values using [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). SGD works by taking the gradient of a random point in our data set and using its value to inform whether to increase or decrease the value of the model weights.\n",
        "\n",
        "It is not an efficient method for this curve fitting problem; this is only for the purpose of demonstrating how an iterative method can be implemented in TensorFlow."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x2TDAsD-BKf_",
        "outputId": "1a0cb607-20ca-496a-bccc-04439a72cb2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        }
      },
      "cell_type": "code",
      "source": [
        "# build a new graph\n",
        "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
        "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
        "\n",
        "deg = 3\n",
        "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
        "\n",
        "# first declare variables that need optimisation\n",
        "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
        "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
        "\n",
        "# # complete the computation graph with SGD\n",
        "# node_1t = tf.matmul(node_X, var_w)\n",
        "# # define a square loss function\n",
        "# loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
        "# # buiding a train-op to minimise the loss\n",
        "# train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
        "\n",
        "# # launch a session\n",
        "# sess = tf.Session()  \n",
        "# sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
        "\n",
        "# # iteration to update variables with backprop gradients\n",
        "# total_iter = int(1e4)\n",
        "# indices_train = [i for i in range(n)]\n",
        "# for step in range(total_iter):\n",
        "\n",
        "#     idx = step % n\n",
        "#     if idx == 0:  # shuffle every epoch\n",
        "#         random.shuffle(indices_train)\n",
        "    \n",
        "#     # single data point feed\n",
        "#     singleDataFeed = {\n",
        "#         ph_1x:x[indices_train[idx],np.newaxis], \n",
        "#         ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
        "    \n",
        "#     # print for testing\n",
        "#     #print(singleDataFeed)\n",
        "    \n",
        "#     # update the variables\n",
        "#     sess.run(train_op, feed_dict=singleDataFeed)\n",
        "    \n",
        "#     # print training information\n",
        "#     if (step % 200) == 0:\n",
        "#         loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
        "#         print('Step %d: Loss=%f' % (step, loss_train))\n",
        "#     if (step % 2000) == 0:\n",
        "#         w_sgd = sess.run(var_w)\n",
        "#         print('Estimated weights:')\n",
        "#         print(w_sgd)\n",
        "\n",
        "# w_sgd = sess.run(var_w)\n",
        "# print('Final weights at step %d:' % step)\n",
        "# print(w_sgd)\n",
        "# sess.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5b1108c2978c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# first declare variables that need optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m var_w = tf.get_variable('weights', shape=[deg+1,1], \n\u001b[0;32m----> 9\u001b[0;31m                         initializer=tf.random_normal_initializer(0, 1e-3))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# # complete the computation graph with SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-4-654fbbb476b8>\", line 9, in <module>\n    initializer=tf.random_normal_initializer(0, 1e-3))\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Qkqm5TFV-JtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        },
        "outputId": "595b9f86-231b-4057-feaa-dc551f4fece0"
      },
      "cell_type": "code",
      "source": [
        "# complete the computation graph with SGD\n",
        "node_1t = tf.matmul(node_X, var_w)\n",
        "# define a square loss function\n",
        "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
        "# buiding a train-op to minimise the loss\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
        "\n",
        "# launch a session\n",
        "sess = tf.Session()  \n",
        "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
        "\n",
        "# iteration to update variables with backprop gradients\n",
        "total_iter = int(1e4)\n",
        "indices_train = [i for i in range(n)]\n",
        "for step in range(total_iter):\n",
        "\n",
        "    idx = step % n\n",
        "    if idx == 0:  # shuffle every epoch\n",
        "        random.shuffle(indices_train)\n",
        "    \n",
        "    # single data point feed\n",
        "    singleDataFeed = {\n",
        "        ph_1x:x[indices_train[idx],np.newaxis], \n",
        "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
        "    \n",
        "    # print for testing\n",
        "    #print(singleDataFeed)\n",
        "    \n",
        "    # update the variables\n",
        "    sess.run(train_op, feed_dict=singleDataFeed)\n",
        "    \n",
        "    # print training information\n",
        "    if (step % 200) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
        "        print('Step %d: Loss=%f' % (step, loss_train))\n",
        "    if (step % 2000) == 0:\n",
        "        w_sgd = sess.run(var_w)\n",
        "        print('Estimated weights:')\n",
        "        print(w_sgd)\n",
        "\n",
        "w_sgd = sess.run(var_w)\n",
        "print('Final weights at step %d:' % step)\n",
        "print(w_sgd)\n",
        "sess.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0: Loss=12.568729\n",
            "Estimated weights:\n",
            "[[0.59463495]\n",
            " [0.76435673]\n",
            " [0.98188186]\n",
            " [1.2611164 ]]\n",
            "Step 200: Loss=0.020210\n",
            "Step 400: Loss=0.075130\n",
            "Step 600: Loss=0.003327\n",
            "Step 800: Loss=0.023082\n",
            "Step 1000: Loss=0.054252\n",
            "Step 1200: Loss=0.013319\n",
            "Step 1400: Loss=0.043618\n",
            "Step 1600: Loss=0.018131\n",
            "Step 1800: Loss=0.084470\n",
            "Step 2000: Loss=0.001207\n",
            "Estimated weights:\n",
            "[[4.0472703]\n",
            " [2.9199054]\n",
            " [1.9787805]\n",
            " [1.0194025]]\n",
            "Step 2200: Loss=0.002373\n",
            "Step 2400: Loss=0.088748\n",
            "Step 2600: Loss=0.002565\n",
            "Step 2800: Loss=0.000068\n",
            "Step 3000: Loss=0.006142\n",
            "Step 3200: Loss=0.003426\n",
            "Step 3400: Loss=0.001752\n",
            "Step 3600: Loss=0.016715\n",
            "Step 3800: Loss=0.013774\n",
            "Step 4000: Loss=0.000079\n",
            "Estimated weights:\n",
            "[[3.9630163 ]\n",
            " [2.9695551 ]\n",
            " [1.9255192 ]\n",
            " [0.98675376]]\n",
            "Step 4200: Loss=0.000119\n",
            "Step 4400: Loss=0.002625\n",
            "Step 4600: Loss=0.000093\n",
            "Step 4800: Loss=0.006856\n",
            "Step 5000: Loss=0.038958\n",
            "Step 5200: Loss=0.064046\n",
            "Step 5400: Loss=0.001451\n",
            "Step 5600: Loss=0.000171\n",
            "Step 5800: Loss=0.015023\n",
            "Step 6000: Loss=0.011663\n",
            "Estimated weights:\n",
            "[[4.1176476]\n",
            " [3.0150716]\n",
            " [1.9400885]\n",
            " [0.9630227]]\n",
            "Step 6200: Loss=0.002027\n",
            "Step 6400: Loss=0.000858\n",
            "Step 6600: Loss=0.002376\n",
            "Step 6800: Loss=0.000523\n",
            "Step 7000: Loss=0.019224\n",
            "Step 7200: Loss=0.004625\n",
            "Step 7400: Loss=0.000274\n",
            "Step 7600: Loss=0.013457\n",
            "Step 7800: Loss=0.017637\n",
            "Step 8000: Loss=0.011067\n",
            "Estimated weights:\n",
            "[[3.9984019]\n",
            " [2.9956398]\n",
            " [1.9255686]\n",
            " [1.0458173]]\n",
            "Step 8200: Loss=0.004608\n",
            "Step 8400: Loss=0.015380\n",
            "Step 8600: Loss=0.045718\n",
            "Step 8800: Loss=0.043947\n",
            "Step 9000: Loss=0.000064\n",
            "Step 9200: Loss=0.016578\n",
            "Step 9400: Loss=0.068597\n",
            "Step 9600: Loss=0.007427\n",
            "Step 9800: Loss=0.002215\n",
            "Final weights at step 9999:\n",
            "[[4.0258517]\n",
            " [2.8983922]\n",
            " [1.9022561]\n",
            " [0.9634182]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "db07WaJ0v1Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Playing around with the optimiser\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uDHZJiUGv-X8",
        "colab_type": "code",
        "outputId": "1674e043-c199-4b45-b8f7-ce88f2cb5eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "\n",
        "# get ground-truth data from the \"true\" model \n",
        "n = 100 \n",
        "w = [4, 3, 2, 1]\n",
        "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
        "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
        "                       np.linspace(len(w)-1,0,len(w))), w)\n",
        "std_noise = 0.2\n",
        "t_observed = np.reshape(\n",
        "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
        "    [-1,1])\n",
        "\n",
        "loss_val = np.array([])\n",
        "\n",
        "# build a new graph\n",
        "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
        "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
        "\n",
        "deg = 3\n",
        "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
        "\n",
        "# first declare variables that need optimisation\n",
        "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
        "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-85b805f43a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# first declare variables that need optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m var_w = tf.get_variable('weights', shape=[deg+1,1], \n\u001b[0;32m---> 29\u001b[0;31m                         initializer=tf.random_normal_initializer(0, 1e-3))\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-4-654fbbb476b8>\", line 9, in <module>\n    initializer=tf.random_normal_initializer(0, 1e-3))\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vjJqISh8_lcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        },
        "outputId": "cfe173de-ae6f-439f-e4ff-9fd81fe0fa11"
      },
      "cell_type": "code",
      "source": [
        "# complete the computation graph with SGD\n",
        "node_1t = tf.matmul(node_X, var_w)\n",
        "# define a square loss function\n",
        "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
        "# buiding a train-op to minimise the loss\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
        "\n",
        "# launch a session\n",
        "sess = tf.Session()  \n",
        "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
        "\n",
        "# iteration to update variables with backprop gradients\n",
        "total_iter = int(1e4)\n",
        "indices_train = [i for i in range(n)]\n",
        "for step in range(total_iter):\n",
        "    \n",
        "    idx = step % n\n",
        "    if idx == 0:  # shuffle every epoch\n",
        "        random.shuffle(indices_train)\n",
        "    \n",
        "    # single data point feed\n",
        "    singleDataFeed = {\n",
        "        ph_1x:x[indices_train[idx],np.newaxis], \n",
        "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
        "    \n",
        "    \n",
        "    # update the variables\n",
        "    sess.run(train_op, feed_dict=singleDataFeed)\n",
        "    \n",
        "    if (step % 1) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
        "        loss_val = np.append(loss_val,loss_train)\n",
        "#         print(loss_train,loss_val)\n",
        "#         display.clear_output()\n",
        "#         plt.plot(loss_val)\n",
        "#         plt.show()\n",
        "#         plt.draw()\n",
        "    \n",
        "    # print training information\n",
        "    if (step % 200) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
        "        print('Step %d: Loss=%f' % (step, loss_train))\n",
        "    if (step % 2000) == 0:\n",
        "        w_sgd = sess.run(var_w)\n",
        "        print('Estimated weights:')\n",
        "        print(w_sgd)\n",
        "\n",
        "w_sgd = sess.run(var_w)\n",
        "print('Final weights at step %d:' % step)\n",
        "print(w_sgd)\n",
        "sess.close()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0: Loss=0.000524\n",
            "Estimated weights:\n",
            "[[-0.00015834]\n",
            " [ 0.0016035 ]\n",
            " [-0.00189796]\n",
            " [ 0.00579561]]\n",
            "Step 200: Loss=0.002952\n",
            "Step 400: Loss=0.017667\n",
            "Step 600: Loss=0.005801\n",
            "Step 800: Loss=0.105830\n",
            "Step 1000: Loss=0.004777\n",
            "Step 1200: Loss=0.004457\n",
            "Step 1400: Loss=0.000233\n",
            "Step 1600: Loss=0.000001\n",
            "Step 1800: Loss=0.033103\n",
            "Step 2000: Loss=0.004255\n",
            "Estimated weights:\n",
            "[[3.9869955]\n",
            " [3.0759559]\n",
            " [2.0708253]\n",
            " [1.0543575]]\n",
            "Step 2200: Loss=0.088015\n",
            "Step 2400: Loss=0.000956\n",
            "Step 2600: Loss=0.000161\n",
            "Step 2800: Loss=0.006362\n",
            "Step 3000: Loss=0.007608\n",
            "Step 3200: Loss=0.005370\n",
            "Step 3400: Loss=0.019030\n",
            "Step 3600: Loss=0.047488\n",
            "Step 3800: Loss=0.080505\n",
            "Step 4000: Loss=0.013774\n",
            "Estimated weights:\n",
            "[[3.9725544]\n",
            " [2.9878843]\n",
            " [2.0620942]\n",
            " [0.9932699]]\n",
            "Step 4200: Loss=0.000793\n",
            "Step 4400: Loss=0.031846\n",
            "Step 4600: Loss=0.021458\n",
            "Step 4800: Loss=0.001150\n",
            "Step 5000: Loss=0.003445\n",
            "Step 5200: Loss=0.000197\n",
            "Step 5400: Loss=0.009190\n",
            "Step 5600: Loss=0.002325\n",
            "Step 5800: Loss=0.029196\n",
            "Step 6000: Loss=0.000763\n",
            "Estimated weights:\n",
            "[[3.9874277]\n",
            " [2.982724 ]\n",
            " [2.0057433]\n",
            " [1.0792596]]\n",
            "Step 6200: Loss=0.019895\n",
            "Step 6400: Loss=0.003949\n",
            "Step 6600: Loss=0.004529\n",
            "Step 6800: Loss=0.001167\n",
            "Step 7000: Loss=0.000916\n",
            "Step 7200: Loss=0.000719\n",
            "Step 7400: Loss=0.019790\n",
            "Step 7600: Loss=0.000341\n",
            "Step 7800: Loss=0.006685\n",
            "Step 8000: Loss=0.008150\n",
            "Estimated weights:\n",
            "[[3.9365242 ]\n",
            " [2.8992615 ]\n",
            " [2.0389428 ]\n",
            " [0.92279595]]\n",
            "Step 8200: Loss=0.112535\n",
            "Step 8400: Loss=0.023947\n",
            "Step 8600: Loss=0.069501\n",
            "Step 8800: Loss=0.001768\n",
            "Step 9000: Loss=0.031366\n",
            "Step 9200: Loss=0.124278\n",
            "Step 9400: Loss=0.000728\n",
            "Step 9600: Loss=0.002482\n",
            "Step 9800: Loss=0.020021\n",
            "Final weights at step 9999:\n",
            "[[4.026138 ]\n",
            " [3.004672 ]\n",
            " [2.0633879]\n",
            " [1.0109589]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bccn8h7eDI_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "f96fa9dc-6dc0-47e5-b46a-d979929cf6ad"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(loss_val)\n",
        "plt.yscale('log')\n",
        "loss_val"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.00540179e-01, 1.60094583e+00, 1.59197852e-01, ...,\n",
              "       1.72654241e-02, 4.12546331e-04, 3.36744054e-03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFKCAYAAADWhMzpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np9dBEx8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bdZPkiAZHAhk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Playing around with more than one point at a time"
      ]
    },
    {
      "metadata": {
        "id": "qZt4IKivG3Kb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7b43760a-e125-4b8d-b920-b657b959b4a0"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# get ground-truth data from the \"true\" model \n",
        "n = 100 \n",
        "w = [4, 3, 2, 1]\n",
        "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
        "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
        "                       np.linspace(len(w)-1,0,len(w))), w)\n",
        "std_noise = 0.2\n",
        "t_observed = np.reshape(\n",
        "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
        "    [-1,1])\n",
        "\n",
        "loss_val = np.array([])\n",
        "\n",
        "# build a new graph\n",
        "ph_nx = tf.placeholder(tf.float32, [n, 1])\n",
        "ph_nt = tf.placeholder(tf.float32, [n, 1])\n",
        "\n",
        "deg = 3\n",
        "node_nX = tf.pow(ph_nx, tf.linspace(tf.to_float(deg),0,deg+1))\n",
        "\n",
        "# first declare variables that need optimisation\n",
        "var_w_n = tf.get_variable('weights', shape=[deg+1,1], \n",
        "                        initializer=tf.random_normal_initializer(0, 1e-3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-6df2fa397266>:23: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qffv2oFzGe0C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        },
        "outputId": "0566fb33-da84-42d4-8ad1-b00be7cf2d9c"
      },
      "cell_type": "code",
      "source": [
        "# complete the computation graph with SGD\n",
        "node_nt = tf.matmul(node_nX, var_w_n)\n",
        "# define a square loss function\n",
        "loss = tf.reduce_mean(tf.square(node_nt-ph_nt))\n",
        "# buiding a train-op to minimise the loss\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
        "\n",
        "\n",
        "# launch a session\n",
        "sess = tf.Session()  \n",
        "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
        "\n",
        "# iteration to update variables with backprop gradients\n",
        "total_iter = int(1e4)\n",
        "indices_train = [i for i in range(n)]\n",
        "for step in range(total_iter):\n",
        "    \n",
        "    idx = step % n\n",
        "    if idx == 0:  # shuffle every epoch\n",
        "        random.shuffle(indices_train)\n",
        "    \n",
        "    # single data point feed\n",
        "    nDataFeed = {\n",
        "        ph_nx:x[indices_train], \n",
        "        ph_nt:t_observed[indices_train] }\n",
        "    \n",
        "    \n",
        "    # update the variables\n",
        "    sess.run(train_op, feed_dict=nDataFeed)\n",
        "    \n",
        "    if (step % 1) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=nDataFeed)\n",
        "        loss_val = np.append(loss_val,loss_train)\n",
        "#         print(loss_train,loss_val)\n",
        "#         display.clear_output()\n",
        "#         plt.plot(loss_val)\n",
        "#         plt.show()\n",
        "#         plt.draw()\n",
        "    \n",
        "    # print training information\n",
        "    if (step % 200) == 0:\n",
        "        loss_train = sess.run(loss, feed_dict=nDataFeed)\n",
        "        print('Step %d: Loss=%f' % (step, loss_train))\n",
        "    if (step % 2000) == 0:\n",
        "        w_sgd = sess.run(var_w_n)\n",
        "        print('Estimated weights:')\n",
        "        print(w_sgd)\n",
        "\n",
        "w_sgd = sess.run(var_w_n)\n",
        "print('Final weights at step %d:' % step)\n",
        "print(w_sgd)\n",
        "sess.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0: Loss=8.957877\n",
            "Estimated weights:\n",
            "[[0.20210573]\n",
            " [0.19630885]\n",
            " [0.29808432]\n",
            " [0.41059282]]\n",
            "Step 200: Loss=0.076045\n",
            "Step 400: Loss=0.057459\n",
            "Step 600: Loss=0.053000\n",
            "Step 800: Loss=0.051892\n",
            "Step 1000: Loss=0.051618\n",
            "Step 1200: Loss=0.051549\n",
            "Step 1400: Loss=0.051532\n",
            "Step 1600: Loss=0.051528\n",
            "Step 1800: Loss=0.051527\n",
            "Step 2000: Loss=0.051527\n",
            "Estimated weights:\n",
            "[[4.043354 ]\n",
            " [2.9940155]\n",
            " [1.917656 ]\n",
            " [1.0286484]]\n",
            "Step 2200: Loss=0.051527\n",
            "Step 2400: Loss=0.051527\n",
            "Step 2600: Loss=0.051527\n",
            "Step 2800: Loss=0.051527\n",
            "Step 3000: Loss=0.051527\n",
            "Step 3200: Loss=0.051527\n",
            "Step 3400: Loss=0.051527\n",
            "Step 3600: Loss=0.051527\n",
            "Step 3800: Loss=0.051527\n",
            "Step 4000: Loss=0.051527\n",
            "Estimated weights:\n",
            "[[4.045168 ]\n",
            " [2.9940155]\n",
            " [1.9164847]\n",
            " [1.0286484]]\n",
            "Step 4200: Loss=0.051527\n",
            "Step 4400: Loss=0.051527\n",
            "Step 4600: Loss=0.051527\n",
            "Step 4800: Loss=0.051527\n",
            "Step 5000: Loss=0.051527\n",
            "Step 5200: Loss=0.051527\n",
            "Step 5400: Loss=0.051527\n",
            "Step 5600: Loss=0.051527\n",
            "Step 5800: Loss=0.051527\n",
            "Step 6000: Loss=0.051527\n",
            "Estimated weights:\n",
            "[[4.045168 ]\n",
            " [2.9940155]\n",
            " [1.9164847]\n",
            " [1.0286484]]\n",
            "Step 6200: Loss=0.051527\n",
            "Step 6400: Loss=0.051527\n",
            "Step 6600: Loss=0.051527\n",
            "Step 6800: Loss=0.051527\n",
            "Step 7000: Loss=0.051527\n",
            "Step 7200: Loss=0.051527\n",
            "Step 7400: Loss=0.051527\n",
            "Step 7600: Loss=0.051527\n",
            "Step 7800: Loss=0.051527\n",
            "Step 8000: Loss=0.051527\n",
            "Estimated weights:\n",
            "[[4.045168 ]\n",
            " [2.9940155]\n",
            " [1.9164847]\n",
            " [1.0286484]]\n",
            "Step 8200: Loss=0.051527\n",
            "Step 8400: Loss=0.051527\n",
            "Step 8600: Loss=0.051527\n",
            "Step 8800: Loss=0.051527\n",
            "Step 9000: Loss=0.051527\n",
            "Step 9200: Loss=0.051527\n",
            "Step 9400: Loss=0.051527\n",
            "Step 9600: Loss=0.051527\n",
            "Step 9800: Loss=0.051527\n",
            "Final weights at step 9999:\n",
            "[[4.045168 ]\n",
            " [2.9940155]\n",
            " [1.9164847]\n",
            " [1.0286484]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gC8pbyVmJ15c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "outputId": "e115d845-025d-441f-a709-69d3de001145"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_val[::])\n",
        "plt.yscale('log')\n",
        "loss_val[:100:]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.96591282, 8.96162033, 8.95787716, 6.84207344, 5.34344864,\n",
              "       4.25572634, 3.44780445, 2.8350184 , 2.36165905, 1.99028122,\n",
              "       1.69512713, 1.4580456 , 1.26593745, 1.10913396, 0.98035133,\n",
              "       0.87400317, 0.78574198, 0.71214247, 0.65047973, 0.59856939,\n",
              "       0.55465031, 0.51729625, 0.48534772, 0.45786023, 0.4340623 ,\n",
              "       0.41332304, 0.39512566, 0.37904626, 0.36473691, 0.35191199,\n",
              "       0.34033638, 0.3298164 , 0.32019246, 0.31133288, 0.30312881,\n",
              "       0.29549024, 0.28834236, 0.28162313, 0.27528092, 0.26927242,\n",
              "       0.26356152, 0.25811774, 0.25291553, 0.247933  , 0.24315184,\n",
              "       0.23855606, 0.23413204, 0.22986801, 0.22575372, 0.22178009,\n",
              "       0.21793924, 0.21422401, 0.21062812, 0.20714588, 0.20377208,\n",
              "       0.20050205, 0.19733131, 0.19425602, 0.19127233, 0.18837683,\n",
              "       0.18556617, 0.18283735, 0.18018752, 0.17761385, 0.17511368,\n",
              "       0.17268459, 0.17032419, 0.1680302 , 0.16580045, 0.1636328 ,\n",
              "       0.16152526, 0.15947588, 0.15748288, 0.15554437, 0.15365858,\n",
              "       0.15182397, 0.15003884, 0.14830166, 0.1466109 , 0.14496519,\n",
              "       0.14336301, 0.14180312, 0.14028408, 0.13880469, 0.13736373,\n",
              "       0.13596001, 0.13459234, 0.13325971, 0.13196091, 0.13069502,\n",
              "       0.12946093, 0.12825775, 0.12708448, 0.12594028, 0.12482418,\n",
              "       0.12373535, 0.12267306, 0.12163635, 0.12062453, 0.11963688])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFKCAYAAADWhMzpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIQINAEAWItAAAGQhAg0AQBb6f/8oEpOSWImwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FxVSxLyeVdXb"
      },
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "- What happens to the convergence when you try other optimisation hyperparameters, such as a different optimiser, learning rate and number of iterations? You could try plotting the loss as a function of step for each hypermarater. \n",
        "- Try adding regularisers and different loss functions. How does this affect the performance?\n",
        "- Would batch gradient descent or minibatch gradient descent improve the optimisation?\n",
        "- Would higher-degree models be more prone to overfitting and why?\n"
      ]
    },
    {
      "metadata": {
        "id": "7BsWVetqrUSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}