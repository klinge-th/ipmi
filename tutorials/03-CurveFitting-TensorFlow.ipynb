{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u6-He3cXBKfM"
   },
   "source": [
    "# 3 Linear Regression - Curve Fitting (TensorFlow)\n",
    "In this tutorial we will use two methods to fit a curve using [TensorFlow](https://www.tensorflow.org/):\n",
    "- Direct solution using the least-squares method - this is the same method used in the previous tutorial with NumPy \n",
    "- Iterative optimisation using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pU5v5NAxBKfQ"
   },
   "source": [
    "## 3.1 Data\n",
    "First, as before, we sample $n$ observed data from the underlying polynomial defined by weights $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "i2T4JW1SBKfU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# get ground-truth data from the \"true\" model \n",
    "n = 100 \n",
    "w = [4, 3, 2, 1]\n",
    "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
    "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
    "                       np.linspace(len(w)-1,0,len(w))), w)\n",
    "std_noise = 0.2\n",
    "t_observed = np.reshape(\n",
    "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
    "    [-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmgdFSrvBKfc"
   },
   "source": [
    "## 3.2 Computation Graph and Session\n",
    "[Graphs and sessions](https://www.tensorflow.org/guide/graphs) are important features of TensorFlow. Briefly:\n",
    "1. a **graph** needs to be built to specify what computations are required; the graphs are made up of nodes (mathematical operations) and edges (multidimensional data arrays, i.e. *tensors*, that are consumed or produced by a computation)\n",
    "1. **sessions** are then constructed to specify what computation to run -  e.g., what data to use and in what order. \n",
    "\n",
    "To facilitate the data feeding, [**placeholders**](https://www.tensorflow.org/api_docs/python/tf/placeholder) are used. The following two methods to fit the model provide two examples of how these are used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gx0uDEF6rURs"
   },
   "source": [
    "First, we build a computation graph using \"tf functions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "8w3xBytsBKff",
    "outputId": "ad257b69-2dac-485f-dbaa-f197b0073b9c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# placeholders are used for feeding data in runtime\n",
    "ph_x = tf.placeholder(tf.float32, [n, 1])\n",
    "ph_t = tf.placeholder(tf.float32, [n, 1])\n",
    "\n",
    "deg = 3\n",
    "# define the computation node X\n",
    "node_X = tf.pow(ph_x, tf.linspace(tf.to_float(deg),0,deg+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82XOeCIErURw"
   },
   "source": [
    "This above is a very simple computation graph to evaluate the polynomial using TensorFlow functions. This can be built without any real data and there has not been any computation taking place either.  \n",
    "\n",
    "Then we construct a session. And, call the run method to evaluate the node *node_X* to actually run the computation and obtain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUHDEl-yBKft"
   },
   "source": [
    "## 3.3 Least-Squares Solution\n",
    "\n",
    "To begin this curve fitting tutorial, we will start with the same method from the previous NumPy tutorial: the least squares solution. The advantages of using TensorFlow are not obvious with this method, however they will become apparent later for SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "DYZu3MyaBKfw",
    "outputId": "c62da439-ffb1-4bad-8c44-0c5f79a8f94a"
   },
   "outputs": [],
   "source": [
    "# we complete the computation graph with the least-square solution\n",
    "node_w = tf.matrix_solve_ls(node_X, ph_t)\n",
    "\n",
    "# run the session to evaluate the node weights\n",
    "sess = tf.Session()  \n",
    "dataFeed = {ph_x:x, ph_t:t_observed}  # feed data\n",
    "w_lstsq = sess.run(node_w, feed_dict=dataFeed)\n",
    "print(w_lstsq)\n",
    "#tf.reset_default_graph()\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhy2ym8FBKf8"
   },
   "source": [
    "## 3.3 Stochastic Gradient Descend Method\n",
    "Instead of using least-squares, weights can be optimised by minimising a loss function between the predicted- and observed target values using [SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). SGD works by taking the gradient of a random point in our data set and using its value to inform whether to increase or decrease the value of the model weights.\n",
    "\n",
    "It is not an efficient method for this curve fitting problem; this is only for the purpose of demonstrating how an iterative method can be implemented in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1476
    },
    "colab_type": "code",
    "id": "x2TDAsD-BKf_",
    "outputId": "23cdb487-2e5a-45db-d32d-a0a5c3b3b540"
   },
   "outputs": [],
   "source": [
    "# build a new graph\n",
    "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
    "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
    "\n",
    "deg = 3\n",
    "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
    "\n",
    "# first declare variables that need optimisation\n",
    "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
    "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
    "\n",
    "# complete the computation graph with SGD\n",
    "node_1t = tf.matmul(node_X, var_w)\n",
    "# define a square loss function\n",
    "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
    "# buiding a train-op to minimise the loss\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# launch a session\n",
    "sess = tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
    "\n",
    "# iteration to update variables with backprop gradients\n",
    "total_iter = int(1e4)\n",
    "indices_train = [i for i in range(n)]\n",
    "for step in range(total_iter):\n",
    "\n",
    "    idx = step % n\n",
    "    if idx == 0:  # shuffle every epoch\n",
    "        random.shuffle(indices_train)\n",
    "    \n",
    "    # single data point feed\n",
    "    singleDataFeed = {\n",
    "        ph_1x:x[indices_train[idx],np.newaxis], \n",
    "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
    "    \n",
    "    # print for testing\n",
    "    #print(singleDataFeed)\n",
    "    \n",
    "    # update the variables\n",
    "    sess.run(train_op, feed_dict=singleDataFeed)\n",
    "    \n",
    "    # print training information\n",
    "    if (step % 200) == 0:\n",
    "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
    "        print('Step %d: Loss=%f' % (step, loss_train))\n",
    "    if (step % 2000) == 0:\n",
    "        w_sgd = sess.run(var_w)\n",
    "        print('Estimated weights:')\n",
    "        print(w_sgd)\n",
    "\n",
    "w_sgd = sess.run(var_w)\n",
    "print('Final weights at step %d:' % step)\n",
    "print(w_sgd)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db07WaJ0v1Fs"
   },
   "source": [
    "# Playing around with the optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "colab_type": "code",
    "id": "uDHZJiUGv-X8",
    "outputId": "488cd519-752e-47d9-a9c0-2260fb1d78f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Step 0: Loss=1.570971\n",
      "Estimated weights:\n",
      "[[0.0018657 ]\n",
      " [0.00728675]\n",
      " [0.04763701]\n",
      " [0.31491482]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Final weights at step 9:\n",
      "[[1.1129044 ]\n",
      " [0.75512886]\n",
      " [1.8949149 ]\n",
      " [1.2097962 ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# get ground-truth data from the \"true\" model \n",
    "n = 100 \n",
    "w = [4, 3, 2, 1]\n",
    "x = np.linspace(-1,1,n)[:,np.newaxis]\n",
    "t = np.matmul(np.power(np.reshape(x,[-1,1]), \n",
    "                       np.linspace(len(w)-1,0,len(w))), w)\n",
    "std_noise = 0.2\n",
    "t_observed = np.reshape(\n",
    "    [t[idx]+random.gauss(0,std_noise) for idx in range(n)],\n",
    "    [-1,1])\n",
    "\n",
    "loss_val = np.array([])\n",
    "\n",
    "# build a new graph\n",
    "ph_1x = tf.placeholder(tf.float32, [1, 1])\n",
    "ph_1t = tf.placeholder(tf.float32, [1, 1])\n",
    "\n",
    "deg = 3\n",
    "node_X = tf.pow(ph_1x, tf.linspace(tf.to_float(deg),0,deg+1))\n",
    "\n",
    "# first declare variables that need optimisation\n",
    "var_w = tf.get_variable('weights', shape=[deg+1,1], \n",
    "                        initializer=tf.random_normal_initializer(0, 1e-3))\n",
    "\n",
    "# complete the computation graph with SGD\n",
    "node_1t = tf.matmul(node_X, var_w)\n",
    "# define a square loss function\n",
    "loss = tf.reduce_mean(tf.square(node_1t-ph_1t))\n",
    "# buiding a train-op to minimise the loss\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# launch a session\n",
    "sess = tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())  # initialise all the variables\n",
    "\n",
    "# iteration to update variables with backprop gradients\n",
    "total_iter = int(1e1)\n",
    "indices_train = [i for i in range(n)]\n",
    "for step in range(total_iter):\n",
    "\n",
    "    idx = step % n\n",
    "    if idx == 0:  # shuffle every epoch\n",
    "        random.shuffle(indices_train)\n",
    "    \n",
    "    # single data point feed\n",
    "    singleDataFeed = {\n",
    "        ph_1x:x[indices_train[idx],np.newaxis], \n",
    "        ph_1t:t_observed[indices_train[idx],np.newaxis] }\n",
    "    \n",
    "    \n",
    "    # update the variables\n",
    "    sess.run(train_op, feed_dict=singleDataFeed)\n",
    "    \n",
    "    if (step % 1) == 0:\n",
    "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
    "        np.append(loss_val,loss_train)\n",
    "        print(loss_val)\n",
    "        #plt.plot(range(step+1),loss_val)\n",
    "        #plt.draw()\n",
    "    \n",
    "    # print training information\n",
    "    if (step % 200) == 0:\n",
    "        loss_train = sess.run(loss, feed_dict=singleDataFeed)\n",
    "        print('Step %d: Loss=%f' % (step, loss_train))\n",
    "    if (step % 2000) == 0:\n",
    "        w_sgd = sess.run(var_w)\n",
    "        print('Estimated weights:')\n",
    "        print(w_sgd)\n",
    "\n",
    "w_sgd = sess.run(var_w)\n",
    "print('Final weights at step %d:' % step)\n",
    "print(w_sgd)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxVSxLyeVdXb"
   },
   "source": [
    "## Questions\n",
    "- What happens to the convergence when you try other optimisation hyperparameters, such as a different optimiser, learning rate and number of iterations? You could try plotting the loss as a function of step for each hypermarater. \n",
    "- Try adding regularisers and different loss functions. How does this affect the performance?\n",
    "- Would batch gradient descent or minibatch gradient descent improve the optimisation?\n",
    "- Would higher-degree models be more prone to overfitting and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7BsWVetqrUSD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorials_3-CurveFitting-TensorFlow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
